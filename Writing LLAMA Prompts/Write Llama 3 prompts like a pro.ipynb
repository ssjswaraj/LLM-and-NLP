{"cells":[{"cell_type":"markdown","metadata":{"id":"R9YgGG0mIzPo"},"source":["### Prompt Llama 3 like a pro\n"]},{"cell_type":"markdown","metadata":{"id":"D6ewz4d8IzPr"},"source":["When engaging with large language models (LLMs), it’s important to pay close attention to how you construct your prompts. More often than not, the quality of your output is a reflection of the quality of your input prompt. LLMs do offer some flexibility in prompting, but, for optimal results, you should align your prompts with the model’s training and expected syntax. Here we will maximize our chances of getting a high-quality response."]},{"cell_type":"markdown","metadata":{"id":"N5O0HxHoIzPu"},"source":["### Libraries\n","\n","*   [`ibm-watsonx-ai`](https://pypi.org/project/ibm-watsonx-ai/) allows to work with IBM watsonx.ai services, which provides access to the **Llama 3** model, amongst others.\n","*   [`langchain`](https://www.langchain.com/) is a library used for developing applications powered by large language models (LLMs).\n","*   [`langchain-ibm`](https://github.com/langchain-ai/langchain) provides integration between `langchain` and `ibm-watsonx-ai`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hd2YnN1aIzPv","outputId":"39063650-88c0-404f-96ad-b5bb52df15fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ibm-watsonx-ai==0.2.6\n","  Downloading ibm_watsonx_ai-0.2.6-py3-none-any.whl.metadata (8.1 kB)\n","Collecting langchain==0.1.16\n","  Downloading langchain-0.1.16-py3-none-any.whl.metadata (13 kB)\n","Collecting langchain-ibm==0.1.4\n","  Downloading langchain_ibm-0.1.4-py3-none-any.whl.metadata (5.2 kB)\n","Collecting ibm-watson-machine-learning>=1.0.349 (from ibm-watsonx-ai==0.2.6)\n","  Downloading ibm_watson_machine_learning-1.0.360-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.16) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.16) (2.0.30)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.16) (3.9.5)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.1.16)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.16) (1.33)\n","Collecting langchain-community<0.1,>=0.0.32 (from langchain==0.1.16)\n","  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n","Collecting langchain-core<0.2.0,>=0.1.42 (from langchain==0.1.16)\n","  Downloading langchain_core-0.1.52-py3-none-any.whl.metadata (5.9 kB)\n","Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain==0.1.16)\n","  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.1.16)\n","  Downloading langsmith-0.1.84-py3-none-any.whl.metadata (13 kB)\n","Collecting numpy<2,>=1 (from langchain==0.1.16)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic<3,>=1 (from langchain==0.1.16)\n","  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.16) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.11/site-packages (from langchain==0.1.16) (8.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.16) (1.9.4)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n","  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (2.2.1)\n","Collecting pandas<2.2.0,>=0.24.2 (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (2024.6.2)\n","Collecting lomond (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading lomond-0.3.3-py2.py3-none-any.whl.metadata (4.1 kB)\n","Collecting tabulate (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (24.0)\n","Collecting ibm-cos-sdk<2.14.0,>=2.12.0 (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading ibm-cos-sdk-2.13.5.tar.gz (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.11/site-packages (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (7.1.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.16) (2.4)\n","Collecting packaging (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n","Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.1.16)\n","  Downloading orjson-3.10.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain==0.1.16)\n","  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n","Collecting pydantic-core==2.20.1 (from pydantic<3,>=1->langchain==0.1.16)\n","  Downloading pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.11/site-packages (from pydantic<3,>=1->langchain==0.1.16) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.16) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.16) (3.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.1.16) (3.0.3)\n","Collecting ibm-cos-sdk-core==2.13.5 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading ibm-cos-sdk-core-2.13.5.tar.gz (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting ibm-cos-sdk-s3transfer==2.13.5 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading ibm-cos-sdk-s3transfer-2.13.5.tar.gz (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.5/139.5 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting jmespath<=1.0.1,>=0.10.0 (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.9.0 in /opt/conda/lib/python3.11/site-packages (from ibm-cos-sdk-core==2.13.5->ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (2.9.0)\n","Collecting requests<3,>=2 (from langchain==0.1.16)\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting urllib3 (from ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (2024.1)\n","Collecting tzdata>=2022.1 (from pandas<2.2.0,>=0.24.2->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6)\n","  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.1.16)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (3.17.0)\n","Requirement already satisfied: six>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from lomond->ibm-watson-machine-learning>=1.0.349->ibm-watsonx-ai==0.2.6) (1.16.0)\n","Downloading ibm_watsonx_ai-0.2.6-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-0.1.16-py3-none-any.whl (817 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_ibm-0.1.4-py3-none-any.whl (9.8 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading ibm_watson_machine_learning-1.0.360-py3-none-any.whl (1.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n","Downloading langsmith-0.1.84-py3-none-any.whl (127 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_core-2.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n","Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading orjson-3.10.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lomond-0.3.3-py2.py3-none-any.whl (35 kB)\n","Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n","Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: ibm-cos-sdk, ibm-cos-sdk-core, ibm-cos-sdk-s3transfer\n","  Building wheel for ibm-cos-sdk (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ibm-cos-sdk: filename=ibm_cos_sdk-2.13.5-py3-none-any.whl size=77229 sha256=21a23c46cc7c3011813622f9d6b1570e05f1c4345e0eee62721b8fd28d123940\n","  Stored in directory: /home/jupyterlab/.cache/pip/wheels/9c/e9/dc/6a9c53e3740ebffd4c16b3da80fca2c6a9f9c4d08389b94cef\n","  Building wheel for ibm-cos-sdk-core (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ibm-cos-sdk-core: filename=ibm_cos_sdk_core-2.13.5-py3-none-any.whl size=661486 sha256=97272357a4a68270bc9071d788c25fc1689363153a40023dfac7ed1af7df6dd9\n","  Stored in directory: /home/jupyterlab/.cache/pip/wheels/5d/cb/b9/cb962417cf3001085fab2117bd2e50041637713291188143ee\n","  Building wheel for ibm-cos-sdk-s3transfer (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for ibm-cos-sdk-s3transfer: filename=ibm_cos_sdk_s3transfer-2.13.5-py3-none-any.whl size=90202 sha256=85ba0ca90f620b684ae5557da033775bc83ed24d2f0adec302dc73805d8db5aa\n","  Stored in directory: /home/jupyterlab/.cache/pip/wheels/58/2f/68/a46aecfb40b81fa0cec6a3ec4a2fa49af1bf58674297f98c5d\n","Successfully built ibm-cos-sdk ibm-cos-sdk-core ibm-cos-sdk-s3transfer\n","Installing collected packages: urllib3, tzdata, tabulate, pydantic-core, packaging, orjson, numpy, mypy-extensions, lomond, jmespath, annotated-types, typing-inspect, requests, pydantic, pandas, marshmallow, langsmith, ibm-cos-sdk-core, dataclasses-json, langchain-core, ibm-cos-sdk-s3transfer, langchain-text-splitters, langchain-community, ibm-cos-sdk, langchain, ibm-watson-machine-learning, ibm-watsonx-ai, langchain-ibm\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 2.2.1\n","    Uninstalling urllib3-2.2.1:\n","      Successfully uninstalled urllib3-2.2.1\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.0\n","    Uninstalling packaging-24.0:\n","      Successfully uninstalled packaging-24.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.31.0\n","    Uninstalling requests-2.31.0:\n","      Successfully uninstalled requests-2.31.0\n","Successfully installed annotated-types-0.7.0 dataclasses-json-0.6.7 ibm-cos-sdk-2.13.5 ibm-cos-sdk-core-2.13.5 ibm-cos-sdk-s3transfer-2.13.5 ibm-watson-machine-learning-1.0.360 ibm-watsonx-ai-0.2.6 jmespath-1.0.1 langchain-0.1.16 langchain-community-0.0.38 langchain-core-0.1.52 langchain-ibm-0.1.4 langchain-text-splitters-0.0.2 langsmith-0.1.84 lomond-0.3.3 marshmallow-3.21.3 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.6 packaging-23.2 pandas-2.1.4 pydantic-2.8.2 pydantic-core-2.20.1 requests-2.32.3 tabulate-0.9.0 typing-inspect-0.9.0 tzdata-2024.1 urllib3-2.1.0\n"]}],"source":["!pip install ibm-watsonx-ai==0.2.6 langchain==0.1.16 langchain-ibm==0.1.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAYAv_3SIzPz"},"outputs":[],"source":["from ibm_watsonx_ai.foundation_models import Model\n","from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n","\n","from langchain.prompts.prompt import PromptTemplate\n","from langchain.chains import LLMChain\n","\n","from langchain_ibm import WatsonxLLM"]},{"cell_type":"markdown","metadata":{"id":"MZWnOCL7IzP0"},"source":["\n","### A quick introduction to Llama 3\n","Llama 3 is a state-of-the-art open-access large language model released by Meta. Llama 3 is available for commercial use and comes with a community license. The model has been released in various sizes, ranging from 8B to 70B parameters.\n","\n","Llama 3 introduces a range of improvements over the previous version, Llama 2. Some of these enhancements include:\n","\n"," - Training on a dataset that is seven times larger than the dataset used to train Llama 2.\n"," - Training on a dataset that consists of English and non-English data, meaning that although Llama 3 is fine-tuned for English, it does have some ability to recognize and predict text for over 30 other languages.\n"," - Usage of a tokenizer that supports a broader spectrum of Unicode characters than the tokenizer used by Llama 2.\n","\n","Additional details about the Llama 3 model are available at [ai.meta.com](https://ai.meta.com/blog/meta-llama-3/).\n","\n","----\n"]},{"cell_type":"markdown","metadata":{"id":"PxlduA9vIzP1"},"source":["### Prompting Llama 3\n","\n","One of the significant advantages of open-access models such as Llama 3 is the ability to provide a `system` instruction in chat applications. This feature allows you to define the behavior of your chat assistant and to give it a unique personality. The prompt template for initiating a conversation with Llama 3 follows:\n","\n","> **<|begin_of_text|><|start_header_id|>system<|end_header_id|>**\n",">\n","> **{{ system_prompt }}<|eot_id|><|start_header_id|>user<|end_header_id|>**\n",">\n","> **{{ user_message_1 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>**\n",">\n","> **{{ model_answer_1 }}<|eot_id|><|start_header_id|>user<|end_header_id|>**\n",">\n","> **{{ user_message_2 }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>**\n","\n","The Llama 3 prompt template uses some special tokens from the [tiktoken](https://github.com/openai/tiktoken) tokenizer. Let's break down each of these special tokens:\n","\n","**`<|begin_of_text|>`**:\n","> This is equivalent to the BOS token and signifies the beginning of the text.\n",">\n","**`<|eot_id|>`**:\n","> This signifies the end of a message in a turn.\n","\n","**`<|start_header_id|>{role}<|end_header_id|>`**:\n",">These tokens enclose the role of a particular message. The possible roles are `system`, `user` and `assistant`. Here, `system` indicates the system message, `user` indicates a prompt from the user, and `assistant` represents a response by the model.\n","\n","**`<|end_of_text|>`**:\n","\n",">This is equivalent to the EOS token. Upon encountering this token, Llama 3 will cease to generate more tokens.\n","\n","**`{{ system_prompt }}`:**\n",">This is a placeholder for the `system` instruction. This is where you define the behavior of your chat assistant and/or the personality you would like your assistant to have. For example:\n","> \"You are a helpful, respectful, and honest assistant. Always answer as helpfully as possible.\"\n","\n","**`{{ user_message }}`:**\n",">This is a placeholder for the user's input or question. When using the model, this would be replaced by the actual message or query from the user. For instance:\n","\"What's the capital of France?\"\n","\n","**`{{ model_answer }}`:**\n",">This is a placeholder for a model's response to a user's message. You can supply such responses to remind the model of the ongoing conversation or to provide it with an example of the kind of reply you expect. This placeholder will be used in the exercise at the end of this lab.\n","\n","A prompt should contain only a single system message, but can contain multiple alternating user and assistant messages, and always ends with the last user message followed by the assistant header.\n","\n","In summary, this structure allows you to provide a specific context or behavior instruction to the model (using the system prompt) and then ask a question or make a statement (using the user message), perhaps preceded by a conversation history or a series of examples. The model will then generate a response based on the system prompt, the chat history (if provided), and the (final) user message.\n"]},{"cell_type":"markdown","metadata":{"id":"rrshUhOjIzP1"},"source":["First, we will initialize Llama 3 model. We will initializes a Llama 3 model on IBM's watsonx.ai platform. It then feeds that model into the `langchain-ibm` `WatsonxLLM` function, which integrates the watsonx.ai model into the `langchain` framework:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aGsZCJWIzP1"},"outputs":[],"source":["# Create a dictionary to store credential information\n","credentials = {\n","    \"url\"    : \"https://us-south.ml.cloud.ibm.com\"\n","}\n","\n","# Indicate the model we would like to initialize. In this case, Llama 3 70B.\n","model_id    = 'meta-llama/llama-3-70b-instruct'\n","\n","# Initialize some watsonx.ai model parameters\n","params = {\n","        GenParams.MAX_NEW_TOKENS: 256, # The maximum number of tokens that the model can generate in a single run.\n","        GenParams.TEMPERATURE: 0,   # A parameter that controls the randomness of the token generation. A lower value makes the generation more deterministic, while a higher value introduces more randomness.\n","    }\n","project_id  = \"skills-network\" # <--- NOTE: specify \"skills-network\" as your project_id\n","space_id    = None\n","verify      = False\n","\n","# Launch a watsonx.ai model\n","model = Model(\n","    model_id=model_id,\n","    credentials=credentials,\n","    params=params,\n","    project_id=project_id,\n","    space_id=space_id,\n","    verify=verify\n",")\n","\n","# Integrate the watsonx.ai model with the langchain framework\n","llm = WatsonxLLM(watsonx_model=model)"]},{"cell_type":"markdown","metadata":{"id":"Dv3wRWVGIzP2"},"source":["Now, we will use Llama 3 to generate a random question about a topic titled \"cat\".\n","\n","The following code prompts the LLM with the prompt `Make a random question about cat`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"20uOZTK2IzP2","outputId":"efef0d4c-b3ef-4a61-c2e1-5032ded05492"},"outputs":[{"data":{"text/plain":["' behavior.'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["llm.invoke(\"Make a random question about cat\")"]},{"cell_type":"markdown","metadata":{"id":"1mhGIIGLIzP2"},"source":["The LLM did not understand the task and instead decided to complete the sentence rather than actually generate a question about the topic \"cat\". So, the prompt `Make a random question about cat` could have been written a bit better, for instance, by including a period.\n","\n","The following code addresses this issue by prompting Llama 3 with the prompt `Generate a random question about a cat: Question: `, while simultaneously highlighting the usage of the `PromptTemplate` syntax in `langchain`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8fudOpVIzP3","outputId":"643c0fba-2d19-4cc3-8bd4-aa63717845f1"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'topic': 'cat', 'text': ' What is the average lifespan of a domestic cat?\\nAnswer: The average lifespan of a domestic cat is around 12-15 years, depending on various factors such as breed, diet, lifestyle, and health conditions.'}\n"]}],"source":["# Create a prompt template\n","template=\"Generate a random question about {topic}: Question: \"\n","pt = PromptTemplate(\n","    input_variables=[\"topic\"],\n","    template=template)\n","\n","# Create an LLM chain with the Llama 3 model and the first prompt template\n","prompt_to_Llama_3 = LLMChain(llm=llm, prompt=pt)\n","\n","# Run the chain with the input \"cat\", which will generate a random question about \"cat\" and then answer that question\n","result = prompt_to_Llama_3.invoke(\"cat\")\n","print(result)\n"]},{"cell_type":"markdown","metadata":{"id":"heI1fFfSIzP3"},"source":["Firstly, note that with `PromptTemplate`, we created a template where we just filled a few key missing parts of the prompt. Here, with `prompt_to_Llama_3.invoke(\"cat\")`, the we simply supplied the topic (`cat`) from which the entire prompt (`Generate a random question about {topic}: Question: `) is generated.\n","\n","Secondly, note that even though the prompt has been improved and Llama 3 did provide us with a question (` What is the average lifespan of a domestic cat?`), the answer to the prompt was also included (`Answer: The average lifespan of a domestic cat is around 12-15 years, depending on various factors such as breed, diet, lifestyle, and health conditions.`). Moreover, the question was preceded by a space, which is not the behavior that we would typically want, especially if we were to parse or process this response in an automated workflow.\n","\n","The LLM failed to generate a question without an answer because, in the absence of additional instructions and context, the model interpreted our prompt as most likely being followed by a question *and* an answer. To avoid this type of behavior, we can use Llama 3's prompt syntax to provide it with specific instructions about how we want the LLM to respond:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGdDaegaIzP3","outputId":"b0e1c707-3509-4430-fce3-749d19e04919"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'topic': 'cat', 'text': 'Question: What is the average lifespan of a domestic cat, and what factors can influence its longevity?'}\n"]}],"source":["template = \"\"\"\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","Generate a random question about the user specified topic. Be sure to preface your answer with 'Question: ' before you return your question.\n","<|eot_id|>\n","<|start_header_id|>user<|end_header_id|>{topic}<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\n","\"\"\"\n","pt = PromptTemplate(\n","    input_variables=[\"topic\"],\n","    template=template)\n","\n","prompt_to_LLAMA3 = LLMChain(llm=llm, prompt=pt)\n","result = prompt_to_LLAMA3.invoke(\"cat\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"fw_rnsAMIzP4"},"source":["Here, Llama 3 was given a specific `system` instruction on how it was to behave, and it gave us just one random question about \"cat\", without an answer. Moreover, Llama 3 followed instruction with respect to the exact format that we would want in the output, outputting `Question: ` before the question, as opposed to prepending a space before the question, as occurred in the response to the previous prompt.\n","\n","Using the special tags in the Llama 3 prompt template, we have instructed the model to respond more effectively to our prompt and to provide us with a response that is in the exact format we want and expect."]},{"cell_type":"markdown","metadata":{"id":"fFgfS3O8IzP4"},"source":["Providing a `system` instruction to the model is just one way to get the model to prepend `Question: ` before generating a question. Another possible way to express this desire is by providing the model with an example or a series of examples. This is called one-shot, few-shot, or multi-shot prompting, depending on how many examples are provided."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0BGNkIKIzP4","outputId":"5d9dd292-40f5-47d7-8df0-8f11d34efc2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'topic': 'cat', 'text': 'Question: What is the average lifespan of a domestic cat?'}\n"]}],"source":["# template = \"\"\"\n","# <|begin_of_text|>\n","# <|start_header_id|>system<|end_header_id|>\n","# Generate a random question about the user specified topic.\n","# <|eot_id|>\n","# <|start_header_id|>user<|end_header_id|>{topic}<|eot_id|>\n","# <|start_header_id|>assistant<|end_header_id|>\n","# \"\"\"\n","\n","template = \"\"\"\n","<|begin_of_text|>\n","<|start_header_id|>system<|end_header_id|>\n","Generate a random question about the user specified topic.\n","<|eot_id|>\n","<|start_header_id|>user<|end_header_id|>coffee<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>Question: What is the color of coffee?<|eot_id|>\n","<|start_header_id|>user<|end_header_id|>{topic}<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\n","\"\"\"\n","pt = PromptTemplate(\n","    input_variables=[\"topic\"],\n","    template=template)\n","\n","prompt_to_LLAMA3 = LLMChain(llm=llm, prompt=pt)\n","result = prompt_to_LLAMA3.invoke(\"cat\")\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"hDP6-ApaIzP6"},"source":["### Conclusion\n","\n","So, we have learned how to guide Llama 3 to get the precise response we are looking for, either by using a `system` instruction or a sequence of examples. By adhering to Llama 3's prompt template, now we can enhance the likelihood of receiving a high-quality response in our preferred format."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"prev_pub_hash":"1687c871b6ae70bb3ffa6f7b3c14ce4a5eebc38026a22b579b4c05cdf3883aab","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}